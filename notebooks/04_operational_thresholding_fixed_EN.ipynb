{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9691d492",
   "metadata": {},
   "source": [
    "# 04 – Operational Risk Thresholding  \n",
    "## Turning Probabilities into Staff-Usable Decisions (Performance & DS Hybrid)\n",
    "\n",
    "### Objective\n",
    "Models are only useful in practice if their outputs translate into **actionable decisions**.\n",
    "\n",
    "This notebook converts model probabilities into operational rules such as:\n",
    "- *How many players per week do we flag?*\n",
    "- *What recall do we achieve at a manageable alert volume?*\n",
    "- *What threshold should be used for “high risk”?*\n",
    "- *What is the precision among flagged players?*\n",
    "\n",
    "### Key ideas (club context)\n",
    "- Injury prevention workflows are capacity-limited: staff can only intervene on a subset of players.\n",
    "- We treat the model as a **ranking / triage tool** rather than a perfect classifier.\n",
    "- We evaluate thresholds and top‑K strategies with practical metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acc585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Setup ===\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    brier_score_loss,\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"figure.figsize\": (7, 4),\n",
    "    \"axes.grid\": True,\n",
    "    \"grid.alpha\": 0.25,\n",
    "    \"axes.spines.top\": False,\n",
    "    \"axes.spines.right\": False,\n",
    "})\n",
    "\n",
    "def resolve_db_path(filename: str = \"analytics.duckdb\") -> Path:\n",
    "    cwd = Path.cwd()\n",
    "    candidates = [cwd / \"lakehouse\" / filename, cwd.parent / \"lakehouse\" / filename]\n",
    "    for p in candidates:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(\"DuckDB not found. Expected lakehouse/analytics.duckdb\")\n",
    "\n",
    "DB_PATH = resolve_db_path()\n",
    "DB_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71f1733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load data ===\n",
    "with duckdb.connect(str(DB_PATH)) as con:\n",
    "    dfp = con.execute(\"\"\"\n",
    "        SELECT *\n",
    "        FROM player_dataset_predictive\n",
    "        WHERE acwr IS NOT NULL\n",
    "    \"\"\").df()\n",
    "\n",
    "dfp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d39875",
   "metadata": {},
   "source": [
    "## 1) Train/Test split (chronological)\n",
    "We simulate forward prediction: train on earlier matches, evaluate on later matches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdaa422",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"minutes_last_7d\", \"minutes_last_14d\", \"minutes_last_28d\", \"acwr\"]\n",
    "\n",
    "cols = features + [\"high_risk_next\"]\n",
    "if \"match_date\" in dfp.columns:\n",
    "    cols.append(\"match_date\")\n",
    "if \"match_id\" in dfp.columns:\n",
    "    cols.append(\"match_id\")\n",
    "if \"player_id\" in dfp.columns:\n",
    "    cols.append(\"player_id\")\n",
    "\n",
    "d = dfp[cols].dropna().copy()\n",
    "\n",
    "if \"match_date\" in d.columns:\n",
    "    d[\"match_date\"] = pd.to_datetime(d[\"match_date\"], errors=\"coerce\")\n",
    "    d = d.dropna(subset=[\"match_date\"]).sort_values(\"match_date\")\n",
    "\n",
    "cut = int(len(d) * 0.8)\n",
    "train = d.iloc[:cut].copy()\n",
    "test = d.iloc[cut:].copy()\n",
    "\n",
    "X_train = train[features]\n",
    "y_train = train[\"high_risk_next\"].astype(int)\n",
    "X_test = test[features]\n",
    "y_test = test[\"high_risk_next\"].astype(int)\n",
    "\n",
    "(len(train), len(test), y_train.mean(), y_test.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c3b8f7",
   "metadata": {},
   "source": [
    "## 2) Fit models (Logit baseline + HGB)\n",
    "You can use either model operationally. HGB often captures non-linear effects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee32327",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", LogisticRegression(max_iter=2000)),\n",
    "])\n",
    "\n",
    "hgb = HistGradientBoostingClassifier(\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    max_iter=300,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "logit.fit(X_train, y_train)\n",
    "hgb.fit(X_train, y_train)\n",
    "\n",
    "# Probabilities on BOTH train and test:\n",
    "p_logit_train = logit.predict_proba(X_train)[:, 1]\n",
    "p_hgb_train   = hgb.predict_proba(X_train)[:, 1]\n",
    "p_logit_test  = logit.predict_proba(X_test)[:, 1]\n",
    "p_hgb_test    = hgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "def basic_metrics(y_true, p):\n",
    "    return {\n",
    "        \"ROC_AUC\": roc_auc_score(y_true, p),\n",
    "        \"PR_AUC\": average_precision_score(y_true, p),\n",
    "        \"Brier\": brier_score_loss(y_true, p),\n",
    "        \"Prevalence\": float(np.mean(y_true)),\n",
    "    }\n",
    "\n",
    "pd.DataFrame([\n",
    "    {\"Split\": \"TEST\", \"Model\": \"LogisticRegression\", **basic_metrics(y_test, p_logit_test)},\n",
    "    {\"Split\": \"TEST\", \"Model\": \"HistGradientBoosting\", **basic_metrics(y_test, p_hgb_test)},\n",
    "]).sort_values([\"Split\", \"ROC_AUC\"], ascending=[True, False])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79fb510",
   "metadata": {},
   "source": [
    "## 3) Threshold sweep (precision / recall / alert volume)\n",
    "We evaluate a grid of thresholds and compute:\n",
    "- precision, recall, F1\n",
    "- number flagged (alerts)\n",
    "- alert rate (% of players flagged)\n",
    "\n",
    "This is more actionable than ROC alone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3876277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_table(y_true, p, thresholds=None):\n",
    "    \"\"\"Metrics as a function of threshold.\n",
    "    NOTE: Use this on TRAIN for policy design, then APPLY the chosen threshold on TEST for evaluation.\n",
    "    \"\"\"\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0.05, 0.95, 19)\n",
    "    rows = []\n",
    "    n = len(y_true)\n",
    "    for t in thresholds:\n",
    "        pred = (p >= t).astype(int)\n",
    "        rows.append({\n",
    "            \"threshold\": float(t),\n",
    "            \"alerts\": int(pred.sum()),\n",
    "            \"alert_rate\": float(pred.mean()),\n",
    "            \"precision\": precision_score(y_true, pred, zero_division=0),\n",
    "            \"recall\": recall_score(y_true, pred, zero_division=0),\n",
    "            \"f1\": f1_score(y_true, pred, zero_division=0),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# POLICY DESIGN (TRAIN): sweep thresholds to understand trade-offs\n",
    "tab_logit_train = threshold_table(y_train, p_logit_train)\n",
    "tab_hgb_train   = threshold_table(y_train, p_hgb_train)\n",
    "\n",
    "tab_hgb_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1540d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot precision-recall vs threshold (TRAIN - policy design)\n",
    "plt.plot(tab_logit_train[\"threshold\"], tab_logit_train[\"precision\"], marker=\"o\", label=\"precision\")\n",
    "plt.plot(tab_logit_train[\"threshold\"], tab_logit_train[\"recall\"], marker=\"o\", label=\"recall\")\n",
    "plt.title(\"Logistic (TRAIN): Precision/Recall vs Threshold\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(tab_hgb_train[\"threshold\"], tab_hgb_train[\"precision\"], marker=\"o\", label=\"precision\")\n",
    "plt.plot(tab_hgb_train[\"threshold\"], tab_hgb_train[\"recall\"], marker=\"o\", label=\"recall\")\n",
    "plt.title(\"HGB (TRAIN): Precision/Recall vs Threshold\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cdad3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot alert volume vs threshold (TRAIN - policy design)\n",
    "plt.plot(tab_logit_train[\"threshold\"], tab_logit_train[\"alerts\"], marker=\"o\", label=\"Logit alerts (train)\")\n",
    "plt.plot(tab_hgb_train[\"threshold\"], tab_hgb_train[\"alerts\"], marker=\"o\", label=\"HGB alerts (train)\")\n",
    "plt.title(\"Alert Volume vs Threshold (TRAIN)\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"# Flagged players\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07879bd",
   "metadata": {},
   "source": [
    "## 4) Operational policies\n",
    "\n",
    "### Policy A — Fixed threshold\n",
    "Choose a threshold based on acceptable alert volume and desired recall.\n",
    "\n",
    "### Policy B — Top-K per matchday\n",
    "Many clubs prefer ranking: flag the top *K* players per matchday/week.\n",
    "\n",
    "Below we implement both.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e1175c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: confusion matrix at a chosen threshold\n",
    "def cm_at_threshold(y_true, p, t):\n",
    "    pred = (p >= t).astype(int)\n",
    "    cm = confusion_matrix(y_true, pred)\n",
    "    prec = precision_score(y_true, pred, zero_division=0)\n",
    "    rec = recall_score(y_true, pred, zero_division=0)\n",
    "    return cm, prec, rec\n",
    "\n",
    "def threshold_for_target_alert_rate(p_train: np.ndarray, target_rate: float = 0.10) -> float:\n",
    "    \"\"\"Choose threshold on TRAIN so that ~target_rate are flagged (capacity constraint).\n",
    "    Uses the (1-target_rate) quantile of TRAIN probabilities.\n",
    "    \"\"\"\n",
    "    target_rate = float(target_rate)\n",
    "    if not (0 < target_rate < 1):\n",
    "        raise ValueError(\"target_rate must be in (0, 1)\")\n",
    "    # Flagging top target_rate probabilities means threshold at the (1-target_rate) quantile.\n",
    "    return float(np.quantile(p_train, 1 - target_rate))\n",
    "\n",
    "def metrics_at_threshold(y_true, p, t):\n",
    "    pred = (p >= t).astype(int)\n",
    "    return {\n",
    "        \"threshold\": float(t),\n",
    "        \"alerts\": int(pred.sum()),\n",
    "        \"alert_rate\": float(pred.mean()),\n",
    "        \"precision\": float(precision_score(y_true, pred, zero_division=0)),\n",
    "        \"recall\": float(recall_score(y_true, pred, zero_division=0)),\n",
    "        \"f1\": float(f1_score(y_true, pred, zero_division=0)),\n",
    "    }\n",
    "\n",
    "TARGET_ALERT_RATE = 0.10\n",
    "\n",
    "# Pick thresholds on TRAIN (no test optimisation)\n",
    "t_logit = threshold_for_target_alert_rate(p_logit_train, TARGET_ALERT_RATE)\n",
    "t_hgb   = threshold_for_target_alert_rate(p_hgb_train,   TARGET_ALERT_RATE)\n",
    "\n",
    "row_logit_train = metrics_at_threshold(y_train, p_logit_train, t_logit)\n",
    "row_hgb_train   = metrics_at_threshold(y_train, p_hgb_train,   t_hgb)\n",
    "\n",
    "row_logit_test = metrics_at_threshold(y_test, p_logit_test, t_logit)\n",
    "row_hgb_test   = metrics_at_threshold(y_test, p_hgb_test,   t_hgb)\n",
    "\n",
    "t_logit, row_logit_train, row_logit_test, t_hgb, row_hgb_train, row_hgb_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ce5130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices at the TRAIN-chosen thresholds (evaluated on TEST)\n",
    "cmL, precL, recL = cm_at_threshold(y_test, p_logit_test, t_logit)\n",
    "cmH, precH, recH = cm_at_threshold(y_test, p_hgb_test, t_hgb)\n",
    "\n",
    "cmL, precL, recL, cmH, precH, recH\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b1e61a",
   "metadata": {},
   "source": [
    "### Top‑K ranking policy (matchday-level)\n",
    "\n",
    "If `match_id` exists, we can flag the top K players **per match** (or per matchday-like unit).\n",
    "This aligns well with staff workflows (e.g., “flag 3–5 players per match”).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e92ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_policy(df_test: pd.DataFrame, p: np.ndarray, k: int = 5, group_col: str = \"match_id\"):\n",
    "    out = df_test.copy()\n",
    "    out = out.reset_index(drop=True)\n",
    "    out[\"p\"] = p\n",
    "    if group_col not in out.columns:\n",
    "        raise ValueError(f\"{group_col} not found. Available columns: {list(out.columns)[:25]} ...\")\n",
    "    out[\"rank_in_group\"] = out.groupby(group_col)[\"p\"].rank(ascending=False, method=\"first\")\n",
    "    out[\"flag_topk\"] = (out[\"rank_in_group\"] <= k).astype(int)\n",
    "    return out\n",
    "\n",
    "# Only run if match_id exists\n",
    "if \"match_id\" in test.columns:\n",
    "    topk5_hgb = topk_policy(test, p_hgb, k=5, group_col=\"match_id\")\n",
    "    y_true = topk5_hgb[\"high_risk_next\"].astype(int).values\n",
    "    y_pred = topk5_hgb[\"flag_topk\"].values\n",
    "    \n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    alerts = int(y_pred.sum())\n",
    "    groups = topk5_hgb[\"match_id\"].nunique()\n",
    "    \n",
    "    {\"k\": 5, \"groups\": groups, \"alerts_total\": alerts, \"alerts_per_group\": alerts / groups, \"precision\": prec, \"recall\": rec}\n",
    "else:\n",
    "    \"match_id not available: Top-K policy skipped (threshold policies still valid).\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc4c707",
   "metadata": {},
   "source": [
    "## 5) Capacity planning: expected alerts per squad-week\n",
    "\n",
    "In practice, staff capacity matters. We estimate alerts under assumptions:\n",
    "- squad size (e.g., 25 players)\n",
    "- decision cycle frequency (e.g., weekly or per match)\n",
    "\n",
    "We approximate alerts as `alert_rate * squad_size`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dffa75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_alerts_per_squad(alert_rate: float, squad_size: int = 25):\n",
    "    return alert_rate * squad_size\n",
    "\n",
    "policy = pd.DataFrame([\n",
    "    {\n",
    "        \"Model\": \"Logit\",\n",
    "        \"Policy\": f\"Threshold via TRAIN quantile (target={TARGET_ALERT_RATE:.0%} alerts)\",\n",
    "        \"threshold\": t_logit,\n",
    "        \"train_alert_rate\": row_logit_train[\"alert_rate\"],\n",
    "        \"test_alert_rate\": row_logit_test[\"alert_rate\"],\n",
    "        \"test_precision\": row_logit_test[\"precision\"],\n",
    "        \"test_recall\": row_logit_test[\"recall\"],\n",
    "        \"test_f1\": row_logit_test[\"f1\"],\n",
    "        \"expected_alerts_per_25\": expected_alerts_per_squad(row_logit_test[\"alert_rate\"], 25),\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"HGB\",\n",
    "        \"Policy\": f\"Threshold via TRAIN quantile (target={TARGET_ALERT_RATE:.0%} alerts)\",\n",
    "        \"threshold\": t_hgb,\n",
    "        \"train_alert_rate\": row_hgb_train[\"alert_rate\"],\n",
    "        \"test_alert_rate\": row_hgb_test[\"alert_rate\"],\n",
    "        \"test_precision\": row_hgb_test[\"precision\"],\n",
    "        \"test_recall\": row_hgb_test[\"recall\"],\n",
    "        \"test_f1\": row_hgb_test[\"f1\"],\n",
    "        \"expected_alerts_per_25\": expected_alerts_per_squad(row_hgb_test[\"alert_rate\"], 25),\n",
    "    },\n",
    "]).sort_values(\"test_recall\", ascending=False)\n",
    "\n",
    "policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed9193f",
   "metadata": {},
   "source": [
    "## 6) Recommended reporting (for README / staff slide)\n",
    "\n",
    "When presenting to a performance team, focus on:\n",
    "- **Alert volume** (“how many players flagged per cycle?”)\n",
    "- **Recall** (“how many risk cases we catch?”)\n",
    "- **Precision** (“how many flagged are truly high risk?”)\n",
    "- **Calibration** (“is 0.7 actually ~70%?”)\n",
    "\n",
    "This turns modelling into operational decision support.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044366d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy/paste friendly summary (no test-based threshold optimisation)\n",
    "final_summary = pd.DataFrame([\n",
    "    {\n",
    "        \"Model\": \"LogisticRegression\",\n",
    "        \"ROC_AUC_TEST\": roc_auc_score(y_test, p_logit_test),\n",
    "        \"PR_AUC_TEST\": average_precision_score(y_test, p_logit_test),\n",
    "        \"Brier_TEST\": brier_score_loss(y_test, p_logit_test),\n",
    "        f\"Threshold_train(q@{TARGET_ALERT_RATE:.0%})\": t_logit,\n",
    "        \"Test alert_rate\": row_logit_test[\"alert_rate\"],\n",
    "        \"Test Precision@thr\": row_logit_test[\"precision\"],\n",
    "        \"Test Recall@thr\": row_logit_test[\"recall\"],\n",
    "        \"Alerts per 25\": expected_alerts_per_squad(row_logit_test[\"alert_rate\"], 25),\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"HistGradientBoosting\",\n",
    "        \"ROC_AUC_TEST\": roc_auc_score(y_test, p_hgb_test),\n",
    "        \"PR_AUC_TEST\": average_precision_score(y_test, p_hgb_test),\n",
    "        \"Brier_TEST\": brier_score_loss(y_test, p_hgb_test),\n",
    "        f\"Threshold_train(q@{TARGET_ALERT_RATE:.0%})\": t_hgb,\n",
    "        \"Test alert_rate\": row_hgb_test[\"alert_rate\"],\n",
    "        \"Test Precision@thr\": row_hgb_test[\"precision\"],\n",
    "        \"Test Recall@thr\": row_hgb_test[\"recall\"],\n",
    "        \"Alerts per 25\": expected_alerts_per_squad(row_hgb_test[\"alert_rate\"], 25),\n",
    "    },\n",
    "]).sort_values(\"ROC_AUC_TEST\", ascending=False)\n",
    "\n",
    "final_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad09a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: alert-rate stability over time (TEST)\n",
    "if \"match_date\" in test.columns:\n",
    "    tmp = test.copy()\n",
    "    tmp[\"p_hgb\"] = p_hgb_test\n",
    "    tmp[\"alert_hgb\"] = (tmp[\"p_hgb\"] >= t_hgb).astype(int)\n",
    "    tmp[\"week\"] = tmp[\"match_date\"].dt.to_period(\"W\").astype(str)\n",
    "    weekly = tmp.groupby(\"week\").agg(\n",
    "        n=(\"alert_hgb\",\"size\"),\n",
    "        alerts=(\"alert_hgb\",\"sum\"),\n",
    "        alert_rate=(\"alert_hgb\",\"mean\"),\n",
    "        prevalence=(\"high_risk_next\",\"mean\"),\n",
    "        avg_p=(\"p_hgb\",\"mean\"),\n",
    "    ).reset_index()\n",
    "    display(weekly.tail(10))\n",
    "    plt.plot(weekly[\"week\"], weekly[\"alert_rate\"], marker=\"o\")\n",
    "    plt.title(\"HGB: weekly alert rate on TEST (threshold chosen on TRAIN)\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.xlabel(\"Week\")\n",
    "    plt.ylabel(\"Alert rate\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No match_date column found; skipping weekly stability plot.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
