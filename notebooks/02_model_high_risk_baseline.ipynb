{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d9aa294",
   "metadata": {},
   "source": [
    "# 02 – Baseline risk model (Logistic Regression)\n",
    "\n",
    "## Objective\n",
    "Develop an interpretable baseline model predicting `high_risk_next`\n",
    "using competitive workload features.\n",
    "\n",
    "Key principles:\n",
    "\n",
    "- Chronological split\n",
    "- Feature scaling\n",
    "- ROC-AUC & PR-AUC evaluation\n",
    "- Coefficient interpretation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0808abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Setup ===\n",
    "import os\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "\n",
    "def resolve_db_path():\n",
    "    cwd = Path.cwd()\n",
    "    candidates = [\n",
    "        cwd / \"lakehouse\" / \"analytics.duckdb\",\n",
    "        cwd.parent / \"lakehouse\" / \"analytics.duckdb\"\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(\"DuckDB file not found.\")\n",
    "\n",
    "DB_PATH = resolve_db_path()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac8f5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load Data ===\n",
    "with duckdb.connect(str(DB_PATH)) as con:\n",
    "    dfp = con.execute(\"SELECT * FROM player_dataset_predictive WHERE acwr IS NOT NULL\").df()\n",
    "\n",
    "dfp.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd6272e",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "Interpretable competitive workload variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e66cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE = os.getenv(\"FRA_TABLE\", \"player_dataset_predictive_v2\")\n",
    "\n",
    "con = duckdb.connect(DB_PATH, read_only=True)\n",
    "dfp = con.execute(f\"SELECT * FROM {TABLE}\").fetchdf()\n",
    "con.close()\n",
    "\n",
    "print(\"Loaded TABLE:\", TABLE)\n",
    "print(\"Columns:\", len(dfp.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307a3411",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"minutes_last_7d\",\"minutes_last_14d\",\"minutes_last_28d\",\"minutes_last_5_matches\",\"acwr\",\n",
    "    \"minutes_std_last_5_matches\",\"minutes_std_last_10_matches\",\n",
    "    \"delta_7d_14d\",\"delta_14d_28d\",\n",
    "    \"ratio_7d_14d\",\"ratio_14d_28d\",\n",
    "    \"acwr_change\",\n",
    "    \"season_minutes_cum\",\"season_matches_played\",\"season_avg_minutes\",\n",
    "    \"minutes_last_3_matches\",\"season_momentum_3v_season_avg\"\n",
    "]\n",
    "\n",
    "target = \"high_risk_next\"\n",
    "\n",
    "cols = features + [target]\n",
    "\n",
    "if \"match_date\" in dfp.columns:\n",
    "    cols.append(\"match_date\")\n",
    "\n",
    "d = dfp[cols].copy()\n",
    "\n",
    "if \"match_date\" in d.columns:\n",
    "    d = d.sort_values(\"match_date\")\n",
    "\n",
    "cut = int(len(d) * 0.8)\n",
    "train = d.iloc[:cut]\n",
    "test = d.iloc[cut:]\n",
    "\n",
    "X_train = train[features]\n",
    "y_train = train[\"high_risk_next\"].astype(int)\n",
    "\n",
    "X_test = test[features]\n",
    "y_test = test[\"high_risk_next\"].astype(int)\n",
    "\n",
    "len(train), len(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c13a96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = [c for c in features if c not in dfp.columns]\n",
    "print(\"Missing:\", missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79420f1b",
   "metadata": {},
   "source": [
    "## Model Evaluation (Chronological Test)\n",
    "\n",
    "- ROC-AUC\n",
    "- PR-AUC\n",
    "- Brier score\n",
    "- Prevalence\n",
    "- Threshold policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd76f38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Model Training ===\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),  # robusto a outliers\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", LogisticRegression(max_iter=2000)),\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "y_proba = pipe.predict_proba(X_test)[:, 1]\n",
    "roc = roc_auc_score(y_test, y_proba)\n",
    "pr  = average_precision_score(y_test, y_proba)\n",
    "brier = brier_score_loss(y_test, y_proba)\n",
    "\n",
    "print(f\"ROC-AUC (test): {roc:.4f}\")\n",
    "print(f\"PR-AUC  (test): {pr:.4f}\")\n",
    "print(f\"Brier  (test): {brier:.4f}\")\n",
    "print(f\"Prevalence (test): {y_test.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88951170",
   "metadata": {},
   "source": [
    "## Probability Calibration & Reliability Analysis\n",
    "\n",
    "- Reliability curve\n",
    "- Expected Calibration Error (ECE)\n",
    "- Operational interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cb427a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# CALIBRATION (Reliability + ECE)\n",
    "# -----------------------\n",
    "def expected_calibration_error(y_true, y_prob, n_bins=10, strategy=\"quantile\"):\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_prob = np.asarray(y_prob).astype(float)\n",
    "\n",
    "    if strategy == \"quantile\":\n",
    "        bins = np.quantile(y_prob, np.linspace(0, 1, n_bins + 1))\n",
    "        bins[0], bins[-1] = 0.0, 1.0\n",
    "    else:\n",
    "        bins = np.linspace(0, 1, n_bins + 1)\n",
    "\n",
    "    bin_ids = np.digitize(y_prob, bins[1:-1], right=True)\n",
    "\n",
    "    ece = 0.0\n",
    "    for b in range(n_bins):\n",
    "        mask = bin_ids == b\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        acc = y_true[mask].mean()\n",
    "        conf = y_prob[mask].mean()\n",
    "        w = mask.mean()\n",
    "        ece += w * abs(acc - conf)\n",
    "    return ece\n",
    "\n",
    "prob_true, prob_pred = calibration_curve(y_test, y_proba, n_bins=10, strategy=\"quantile\")\n",
    "ece = expected_calibration_error(y_test, y_proba, n_bins=10, strategy=\"quantile\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot([0, 1], [0, 1])\n",
    "plt.plot(prob_pred, prob_true, marker=\"o\")\n",
    "plt.title(f\"Calibration curve (TEST) | ECE={ece:.3f}\")\n",
    "plt.xlabel(\"Mean predicted probability\")\n",
    "plt.ylabel(\"Observed frequency\")\n",
    "plt.show()\n",
    "\n",
    "print(\"ECE (test):\", round(ece, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fbc97d",
   "metadata": {},
   "source": [
    "### Calibration analysis (test)\n",
    "\n",
    "The model shows acceptable calibration (ECE ≈ 0.089).  \n",
    "In higher probability bins, observed frequency slightly exceeds predicted probability, indicating mild underestimation of elevated risk.\n",
    "\n",
    "This conservative bias is preferable in operational settings where false alarms carry cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9045c570",
   "metadata": {},
   "source": [
    "## Coefficient interpretation\n",
    "\n",
    "Positive coefficients increase log-odds of elevated risk proxy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0100ed0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_model = pipe.named_steps[\"model\"]\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    \"feature\": features,\n",
    "    \"coef\": logit_model.coef_[0],\n",
    "    \"odds_ratio\": np.exp(logit_model.coef_[0])\n",
    "})\n",
    "\n",
    "coef_df[\"abs_coef\"] = coef_df[\"coef\"].abs()\n",
    "coef_df = coef_df.sort_values(\"abs_coef\", ascending=False)\n",
    "\n",
    "coef_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cd1302",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = 0.10\n",
    "thr = float(np.quantile(pipe.predict_proba(X_train)[:,1], 1-cap))\n",
    "y_pred = (y_proba >= thr).astype(int)\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "print(\"Threshold (train quantile @10%):\", thr)\n",
    "print(\"Test alert rate:\", y_pred.mean())\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, zero_division=0))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, zero_division=0))\n",
    "print(\"F1:\", f1_score(y_test, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d6b69a",
   "metadata": {},
   "source": [
    "## Positioning within the project\n",
    "\n",
    "This notebook establishes the baseline modelling layer.\n",
    "\n",
    "Subsequent notebooks extend this foundation:\n",
    "\n",
    "- Notebook 03 → Model comparison\n",
    "- Notebook 04 → Operational thresholding\n",
    "- Notebook 05 → Rolling deployment validation\n",
    "\n",
    "The baseline selected for operational use is Logistic Regression,\n",
    "due to its interpretability and stable calibration behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee2bad2",
   "metadata": {},
   "source": [
    "## Feature engineering rationale\n",
    "\n",
    "Workload features were expanded to include:\n",
    "\n",
    "- Rolling standard deviations\n",
    "- Short vs medium-term deltas\n",
    "- Season cumulative load\n",
    "- Season momentum relative to baseline\n",
    "\n",
    "The objective was to capture:\n",
    "\n",
    "1. Acute workload shocks\n",
    "2. Chronic load accumulation\n",
    "3. Structural season fatigue\n",
    "4. Load volatility\n",
    "\n",
    "Rolling validation confirmed that interaction ratios\n",
    "provide incremental predictive signal.\n",
    "\n",
    "Feature expansion improved overall ROC-AUC stability\n",
    "compared to the initial baseline specification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
