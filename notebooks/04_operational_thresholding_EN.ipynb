{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9691d492",
   "metadata": {},
   "source": [
    "# 04 – Operational Risk Thresholding  \n",
    "## Turning Probabilities into Staff-Usable Decisions (Performance & DS Hybrid)\n",
    "\n",
    "### Objective\n",
    "Models are only useful in practice if their outputs translate into **actionable decisions**.\n",
    "\n",
    "This notebook converts model probabilities into operational rules such as:\n",
    "- *How many players per week do we flag?*\n",
    "- *What recall do we achieve at a manageable alert volume?*\n",
    "- *What threshold should be used for “high risk”?*\n",
    "- *What is the precision among flagged players?*\n",
    "\n",
    "### Key ideas (club context)\n",
    "- Injury prevention workflows are capacity-limited: staff can only intervene on a subset of players.\n",
    "- We treat the model as a **ranking / triage tool** rather than a perfect classifier.\n",
    "- We evaluate thresholds and top‑K strategies with practical metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acc585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Setup ===\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    brier_score_loss,\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"figure.figsize\": (7, 4),\n",
    "    \"axes.grid\": True,\n",
    "    \"grid.alpha\": 0.25,\n",
    "    \"axes.spines.top\": False,\n",
    "    \"axes.spines.right\": False,\n",
    "})\n",
    "\n",
    "def resolve_db_path(filename: str = \"analytics.duckdb\") -> Path:\n",
    "    cwd = Path.cwd()\n",
    "    candidates = [cwd / \"lakehouse\" / filename, cwd.parent / \"lakehouse\" / filename]\n",
    "    for p in candidates:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(\"DuckDB not found. Expected lakehouse/analytics.duckdb\")\n",
    "\n",
    "DB_PATH = resolve_db_path()\n",
    "DB_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71f1733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load data ===\n",
    "with duckdb.connect(str(DB_PATH)) as con:\n",
    "    dfp = con.execute(\"\"\"\n",
    "        SELECT *\n",
    "        FROM player_dataset_predictive\n",
    "        WHERE acwr IS NOT NULL\n",
    "    \"\"\").df()\n",
    "\n",
    "dfp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d39875",
   "metadata": {},
   "source": [
    "## 1) Train/Test split (chronological)\n",
    "We simulate forward prediction: train on earlier matches, evaluate on later matches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdaa422",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"minutes_last_7d\", \"minutes_last_14d\", \"minutes_last_28d\", \"acwr\"]\n",
    "\n",
    "cols = features + [\"high_risk_next\"]\n",
    "if \"match_date\" in dfp.columns:\n",
    "    cols.append(\"match_date\")\n",
    "if \"match_id\" in dfp.columns:\n",
    "    cols.append(\"match_id\")\n",
    "if \"player_id\" in dfp.columns:\n",
    "    cols.append(\"player_id\")\n",
    "\n",
    "d = dfp[cols].dropna().copy()\n",
    "\n",
    "if \"match_date\" in d.columns:\n",
    "    d[\"match_date\"] = pd.to_datetime(d[\"match_date\"], errors=\"coerce\")\n",
    "    d = d.dropna(subset=[\"match_date\"]).sort_values(\"match_date\")\n",
    "\n",
    "cut = int(len(d) * 0.8)\n",
    "train = d.iloc[:cut].copy()\n",
    "test = d.iloc[cut:].copy()\n",
    "\n",
    "X_train = train[features]\n",
    "y_train = train[\"high_risk_next\"].astype(int)\n",
    "X_test = test[features]\n",
    "y_test = test[\"high_risk_next\"].astype(int)\n",
    "\n",
    "(len(train), len(test), y_train.mean(), y_test.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c3b8f7",
   "metadata": {},
   "source": [
    "## 2) Fit models (Logit baseline + HGB)\n",
    "You can use either model operationally. HGB often captures non-linear effects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee32327",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", LogisticRegression(max_iter=2000)),\n",
    "])\n",
    "\n",
    "hgb = HistGradientBoostingClassifier(\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    max_iter=300,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "logit.fit(X_train, y_train)\n",
    "hgb.fit(X_train, y_train)\n",
    "\n",
    "p_logit = logit.predict_proba(X_test)[:, 1]\n",
    "p_hgb = hgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "def basic_metrics(y_true, p):\n",
    "    return {\n",
    "        \"ROC_AUC\": roc_auc_score(y_true, p),\n",
    "        \"PR_AUC\": average_precision_score(y_true, p),\n",
    "        \"Brier\": brier_score_loss(y_true, p),\n",
    "        \"Prevalence\": float(np.mean(y_true)),\n",
    "    }\n",
    "\n",
    "pd.DataFrame([\n",
    "    {\"Model\": \"LogisticRegression\", **basic_metrics(y_test, p_logit)},\n",
    "    {\"Model\": \"HistGradientBoosting\", **basic_metrics(y_test, p_hgb)},\n",
    "]).sort_values(\"ROC_AUC\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79fb510",
   "metadata": {},
   "source": [
    "## 3) Threshold sweep (precision / recall / alert volume)\n",
    "We evaluate a grid of thresholds and compute:\n",
    "- precision, recall, F1\n",
    "- number flagged (alerts)\n",
    "- alert rate (% of players flagged)\n",
    "\n",
    "This is more actionable than ROC alone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3876277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_table(y_true, p, thresholds=None):\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0.05, 0.95, 19)\n",
    "    rows = []\n",
    "    n = len(y_true)\n",
    "    for t in thresholds:\n",
    "        pred = (p >= t).astype(int)\n",
    "        rows.append({\n",
    "            \"threshold\": t,\n",
    "            \"alerts\": int(pred.sum()),\n",
    "            \"alert_rate\": pred.mean(),\n",
    "            \"precision\": precision_score(y_true, pred, zero_division=0),\n",
    "            \"recall\": recall_score(y_true, pred, zero_division=0),\n",
    "            \"f1\": f1_score(y_true, pred, zero_division=0),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "tab_logit = threshold_table(y_test, p_logit)\n",
    "tab_hgb = threshold_table(y_test, p_hgb)\n",
    "\n",
    "tab_hgb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1540d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot precision-recall vs threshold (Logit)\n",
    "plt.plot(tab_logit[\"threshold\"], tab_logit[\"precision\"], marker=\"o\", label=\"precision\")\n",
    "plt.plot(tab_logit[\"threshold\"], tab_logit[\"recall\"], marker=\"o\", label=\"recall\")\n",
    "plt.title(\"Logistic: Precision/Recall vs Threshold\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot precision-recall vs threshold (HGB)\n",
    "plt.plot(tab_hgb[\"threshold\"], tab_hgb[\"precision\"], marker=\"o\", label=\"precision\")\n",
    "plt.plot(tab_hgb[\"threshold\"], tab_hgb[\"recall\"], marker=\"o\", label=\"recall\")\n",
    "plt.title(\"HGB: Precision/Recall vs Threshold\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cdad3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot alert volume vs threshold\n",
    "plt.plot(tab_logit[\"threshold\"], tab_logit[\"alerts\"], marker=\"o\", label=\"Logit alerts\")\n",
    "plt.plot(tab_hgb[\"threshold\"], tab_hgb[\"alerts\"], marker=\"o\", label=\"HGB alerts\")\n",
    "plt.title(\"Alert Volume vs Threshold\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"# Flagged players\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07879bd",
   "metadata": {},
   "source": [
    "## 4) Operational policies\n",
    "\n",
    "### Policy A — Fixed threshold\n",
    "Choose a threshold based on acceptable alert volume and desired recall.\n",
    "\n",
    "### Policy B — Top-K per matchday\n",
    "Many clubs prefer ranking: flag the top *K* players per matchday/week.\n",
    "\n",
    "Below we implement both.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e1175c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: confusion matrix at a chosen threshold\n",
    "def cm_at_threshold(y_true, p, t):\n",
    "    pred = (p >= t).astype(int)\n",
    "    cm = confusion_matrix(y_true, pred)\n",
    "    prec = precision_score(y_true, pred, zero_division=0)\n",
    "    rec = recall_score(y_true, pred, zero_division=0)\n",
    "    return cm, prec, rec\n",
    "\n",
    "# Example: choose threshold to flag ~10% of players (approx)\n",
    "def pick_threshold_for_alert_rate(tab, target_rate=0.10):\n",
    "    i = (tab[\"alert_rate\"] - target_rate).abs().idxmin()\n",
    "    return float(tab.loc[i, \"threshold\"]), tab.loc[i].to_dict()\n",
    "\n",
    "t_logit, row_logit = pick_threshold_for_alert_rate(tab_logit, target_rate=0.10)\n",
    "t_hgb, row_hgb = pick_threshold_for_alert_rate(tab_hgb, target_rate=0.10)\n",
    "\n",
    "t_logit, row_logit, t_hgb, row_hgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ce5130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices at the 10% alert-rate thresholds\n",
    "cmL, precL, recL = cm_at_threshold(y_test, p_logit, t_logit)\n",
    "cmH, precH, recH = cm_at_threshold(y_test, p_hgb, t_hgb)\n",
    "\n",
    "cmL, precL, recL, cmH, precH, recH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b1e61a",
   "metadata": {},
   "source": [
    "### Top‑K ranking policy (matchday-level)\n",
    "\n",
    "If `match_id` exists, we can flag the top K players **per match** (or per matchday-like unit).\n",
    "This aligns well with staff workflows (e.g., “flag 3–5 players per match”).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e92ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_policy(df_test: pd.DataFrame, p: np.ndarray, k: int = 5, group_col: str = \"match_id\"):\n",
    "    out = df_test.copy()\n",
    "    out = out.reset_index(drop=True)\n",
    "    out[\"p\"] = p\n",
    "    if group_col not in out.columns:\n",
    "        raise ValueError(f\"{group_col} not found. Available columns: {list(out.columns)[:25]} ...\")\n",
    "    out[\"rank_in_group\"] = out.groupby(group_col)[\"p\"].rank(ascending=False, method=\"first\")\n",
    "    out[\"flag_topk\"] = (out[\"rank_in_group\"] <= k).astype(int)\n",
    "    return out\n",
    "\n",
    "# Only run if match_id exists\n",
    "if \"match_id\" in test.columns:\n",
    "    topk5_hgb = topk_policy(test, p_hgb, k=5, group_col=\"match_id\")\n",
    "    y_true = topk5_hgb[\"high_risk_next\"].astype(int).values\n",
    "    y_pred = topk5_hgb[\"flag_topk\"].values\n",
    "    \n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    alerts = int(y_pred.sum())\n",
    "    groups = topk5_hgb[\"match_id\"].nunique()\n",
    "    \n",
    "    {\"k\": 5, \"groups\": groups, \"alerts_total\": alerts, \"alerts_per_group\": alerts / groups, \"precision\": prec, \"recall\": rec}\n",
    "else:\n",
    "    \"match_id not available: Top-K policy skipped (threshold policies still valid).\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc4c707",
   "metadata": {},
   "source": [
    "## 5) Capacity planning: expected alerts per squad-week\n",
    "\n",
    "In practice, staff capacity matters. We estimate alerts under assumptions:\n",
    "- squad size (e.g., 25 players)\n",
    "- decision cycle frequency (e.g., weekly or per match)\n",
    "\n",
    "We approximate alerts as `alert_rate * squad_size`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dffa75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_alerts_per_squad(alert_rate: float, squad_size: int = 25):\n",
    "    return alert_rate * squad_size\n",
    "\n",
    "policy = pd.DataFrame([\n",
    "    {\"Model\": \"Logit\", \"Policy\": \"Threshold ~10% alerts\", \"threshold\": t_logit, \"alert_rate\": row_logit[\"alert_rate\"], \n",
    "     \"precision\": row_logit[\"precision\"], \"recall\": row_logit[\"recall\"], \"alerts_per_25\": expected_alerts_per_squad(row_logit[\"alert_rate\"], 25)},\n",
    "    {\"Model\": \"HGB\", \"Policy\": \"Threshold ~10% alerts\", \"threshold\": t_hgb, \"alert_rate\": row_hgb[\"alert_rate\"], \n",
    "     \"precision\": row_hgb[\"precision\"], \"recall\": row_hgb[\"recall\"], \"alerts_per_25\": expected_alerts_per_squad(row_hgb[\"alert_rate\"], 25)},\n",
    "]).sort_values(\"recall\", ascending=False)\n",
    "\n",
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed9193f",
   "metadata": {},
   "source": [
    "## 6) Recommended reporting (for README / staff slide)\n",
    "\n",
    "When presenting to a performance team, focus on:\n",
    "- **Alert volume** (“how many players flagged per cycle?”)\n",
    "- **Recall** (“how many risk cases we catch?”)\n",
    "- **Precision** (“how many flagged are truly high risk?”)\n",
    "- **Calibration** (“is 0.7 actually ~70%?”)\n",
    "\n",
    "This turns modelling into operational decision support.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044366d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy/paste friendly summary\n",
    "final_summary = pd.DataFrame([\n",
    "    {\"Model\": \"LogisticRegression\", \"ROC_AUC\": roc_auc_score(y_test, p_logit), \"PR_AUC\": average_precision_score(y_test, p_logit), \n",
    "     \"Threshold(~10%)\": t_logit, \"Precision@thr\": row_logit[\"precision\"], \"Recall@thr\": row_logit[\"recall\"], \"Alerts per 25\": expected_alerts_per_squad(row_logit[\"alert_rate\"], 25)},\n",
    "    {\"Model\": \"HistGradientBoosting\", \"ROC_AUC\": roc_auc_score(y_test, p_hgb), \"PR_AUC\": average_precision_score(y_test, p_hgb), \n",
    "     \"Threshold(~10%)\": t_hgb, \"Precision@thr\": row_hgb[\"precision\"], \"Recall@thr\": row_hgb[\"recall\"], \"Alerts per 25\": expected_alerts_per_squad(row_hgb[\"alert_rate\"], 25)},\n",
    "]).sort_values(\"ROC_AUC\", ascending=False)\n",
    "\n",
    "final_summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
