{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "641aa6f9",
   "metadata": {},
   "source": [
    "# 03 – Model Comparison  \n",
    "## Interpretable Baseline vs Non‑Linear Models (Performance & DS Hybrid)\n",
    "\n",
    "### Objective\n",
    "Compare an interpretable baseline (**Logistic Regression**) against a non‑linear model (**HistGradientBoostingClassifier**) to assess whether non‑linear structure in workload features improves predictive performance for `high_risk_next`.\n",
    "\n",
    "### Why this matters (Performance context)\n",
    "Workload–risk relationships are rarely linear (e.g., *sweet spot* vs *spike* patterns).  \n",
    "A non‑linear model can capture thresholds and interactions that a linear model may miss, while the baseline model provides transparency.\n",
    "\n",
    "### Evaluation\n",
    "- Leakage-aware split (chronological)\n",
    "- Metrics: **ROC‑AUC**, **PR‑AUC**\n",
    "- Diagnostics: ROC/PR curves + probability calibration (Brier + reliability plot)\n",
    "- Interpretability: coefficients (logistic) + permutation importance (HGB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f8108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Setup ===\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    brier_score_loss,\n",
    "    RocCurveDisplay,\n",
    "    PrecisionRecallDisplay,\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"figure.figsize\": (7, 4),\n",
    "    \"axes.grid\": True,\n",
    "    \"grid.alpha\": 0.25,\n",
    "    \"axes.spines.top\": False,\n",
    "    \"axes.spines.right\": False,\n",
    "})\n",
    "\n",
    "def resolve_db_path(filename: str = \"analytics.duckdb\") -> Path:\n",
    "    cwd = Path.cwd()\n",
    "    candidates = [\n",
    "        cwd / \"lakehouse\" / filename,\n",
    "        cwd.parent / \"lakehouse\" / filename,\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(\n",
    "        \"DuckDB not found. Expected lakehouse/analytics.duckdb (or ../lakehouse/analytics.duckdb).\"\n",
    "    )\n",
    "\n",
    "DB_PATH = resolve_db_path()\n",
    "DB_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77548819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load data ===\n",
    "with duckdb.connect(str(DB_PATH)) as con:\n",
    "    dfp = con.execute(\"\"\"\n",
    "        SELECT *\n",
    "        FROM player_dataset_predictive\n",
    "        WHERE acwr IS NOT NULL\n",
    "    \"\"\").df()\n",
    "\n",
    "dfp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9358ca",
   "metadata": {},
   "source": [
    "## 1) Feature set\n",
    "We keep a compact, interpretable workload set (competitive load only).  \n",
    "If you later add club‑level data (GPS, sRPE, HRV), this notebook remains valid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afc649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline workload features\n",
    "features = [\"minutes_last_7d\", \"minutes_last_14d\", \"minutes_last_28d\", \"acwr\"]\n",
    "\n",
    "# Include match_date if available to perform chronological split\n",
    "cols = features + [\"high_risk_next\"]\n",
    "if \"match_date\" in dfp.columns:\n",
    "    cols.append(\"match_date\")\n",
    "\n",
    "d = dfp[cols].dropna().copy()\n",
    "\n",
    "# Ensure chronological order if match_date exists\n",
    "if \"match_date\" in d.columns:\n",
    "    d[\"match_date\"] = pd.to_datetime(d[\"match_date\"], errors=\"coerce\")\n",
    "    d = d.dropna(subset=[\"match_date\"]).sort_values(\"match_date\")\n",
    "\n",
    "# Train/test split (80/20, forward)\n",
    "cut = int(len(d) * 0.8)\n",
    "train = d.iloc[:cut].copy()\n",
    "test = d.iloc[cut:].copy()\n",
    "\n",
    "X_train = train[features]\n",
    "y_train = train[\"high_risk_next\"].astype(int)\n",
    "\n",
    "X_test = test[features]\n",
    "y_test = test[\"high_risk_next\"].astype(int)\n",
    "\n",
    "(len(train), len(test), y_train.mean(), y_test.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0c157f",
   "metadata": {},
   "source": [
    "## 2) Models\n",
    "- **Logistic Regression** (scaled) for interpretability  \n",
    "- **HistGradientBoostingClassifier** for non‑linear patterns\n",
    "\n",
    "We keep defaults mostly stable, focusing on correct evaluation first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564b9e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression pipeline (scaling required)\n",
    "logit = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", LogisticRegression(max_iter=2000)),\n",
    "])\n",
    "\n",
    "# Non-linear model (no scaling needed)\n",
    "hgb = HistGradientBoostingClassifier(\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    max_iter=300,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Fit\n",
    "logit.fit(X_train, y_train)\n",
    "hgb.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities\n",
    "p_logit = logit.predict_proba(X_test)[:, 1]\n",
    "p_hgb = hgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "def metrics(y_true, p):\n",
    "    return {\n",
    "        \"roc_auc\": roc_auc_score(y_true, p),\n",
    "        \"pr_auc\": average_precision_score(y_true, p),\n",
    "        \"brier\": brier_score_loss(y_true, p),\n",
    "    }\n",
    "\n",
    "m_logit = metrics(y_test, p_logit)\n",
    "m_hgb = metrics(y_test, p_hgb)\n",
    "\n",
    "pd.DataFrame([\n",
    "    {\"model\": \"LogisticRegression\", **m_logit},\n",
    "    {\"model\": \"HistGradientBoosting\", **m_hgb},\n",
    "]).sort_values(\"roc_auc\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb571d5",
   "metadata": {},
   "source": [
    "## 3) ROC & PR curves\n",
    "We overlay curves for quick comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f925e769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curves\n",
    "RocCurveDisplay.from_predictions(y_test, p_logit, name=f\"Logit (AUC={m_logit['roc_auc']:.3f})\")\n",
    "RocCurveDisplay.from_predictions(y_test, p_hgb, name=f\"HGB (AUC={m_hgb['roc_auc']:.3f})\")\n",
    "plt.title(\"ROC Curve – Model Comparison\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall curves\n",
    "PrecisionRecallDisplay.from_predictions(y_test, p_logit, name=f\"Logit (AP={m_logit['pr_auc']:.3f})\")\n",
    "PrecisionRecallDisplay.from_predictions(y_test, p_hgb, name=f\"HGB (AP={m_hgb['pr_auc']:.3f})\")\n",
    "plt.title(\"Precision‑Recall – Model Comparison\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e7aa4c",
   "metadata": {},
   "source": [
    "## 4) Calibration (reliability)\n",
    "In performance settings, probability calibration matters: staff often act on risk thresholds.\n",
    "We plot calibration curves and report **Brier score** (lower is better).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07518d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_calibration(y_true, p, label):\n",
    "    frac_pos, mean_pred = calibration_curve(y_true, p, n_bins=10, strategy=\"quantile\")\n",
    "    plt.plot(mean_pred, frac_pos, marker=\"o\", label=label)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plot_calibration(y_test, p_logit, f\"Logit (Brier={m_logit['brier']:.3f})\")\n",
    "plot_calibration(y_test, p_hgb, f\"HGB (Brier={m_hgb['brier']:.3f})\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "plt.title(\"Calibration Curve (Reliability Plot)\")\n",
    "plt.xlabel(\"Mean predicted probability\")\n",
    "plt.ylabel(\"Fraction of positives\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07aca54",
   "metadata": {},
   "source": [
    "## 5) Interpretability\n",
    "### Logistic coefficients\n",
    "Coefficients are shown on standardized features (via pipeline). Positive coefficients increase log‑odds of risk.\n",
    "\n",
    "### Permutation importance (HGB)\n",
    "Permutation importance provides a model‑agnostic view of which features matter most for non‑linear models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7755aa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic coefficients (standardized features)\n",
    "coef = logit.named_steps[\"model\"].coef_[0]\n",
    "coef_df = pd.DataFrame({\n",
    "    \"feature\": features,\n",
    "    \"coef\": coef,\n",
    "    \"odds_ratio\": np.exp(coef),\n",
    "}).sort_values(\"coef\", ascending=False)\n",
    "\n",
    "coef_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71eeeeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation importance for HGB (on test set)\n",
    "perm = permutation_importance(\n",
    "    hgb, X_test, y_test,\n",
    "    n_repeats=10,\n",
    "    random_state=42,\n",
    "    scoring=\"roc_auc\"\n",
    ")\n",
    "\n",
    "imp_df = pd.DataFrame({\n",
    "    \"feature\": X_test.columns,\n",
    "    \"importance_mean\": perm.importances_mean,\n",
    "    \"importance_std\": perm.importances_std,\n",
    "}).sort_values(\"importance_mean\", ascending=False)\n",
    "\n",
    "imp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a06c35",
   "metadata": {},
   "source": [
    "## 6) Practical summary (Performance + DS)\n",
    "Use this section to write up conclusions for README / report.\n",
    "\n",
    "Suggested interpretation template:\n",
    "\n",
    "- **Discrimination:** Which model better separates higher-risk cases (ROC/PR)?  \n",
    "- **Calibration:** Which model provides more reliable probabilities (Brier, calibration curve)?  \n",
    "- **Interpretability trade-off:** Logistic is transparent; HGB may improve performance but needs careful explanation.  \n",
    "- **Operational use:** Select thresholds based on acceptable false positives for staff workload management.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e783a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenience summary table (copy/paste into README if needed)\n",
    "summary = pd.DataFrame([\n",
    "    {\"Model\": \"LogisticRegression\", \"ROC_AUC\": m_logit[\"roc_auc\"], \"PR_AUC\": m_logit[\"pr_auc\"], \"Brier\": m_logit[\"brier\"]},\n",
    "    {\"Model\": \"HistGradientBoosting\", \"ROC_AUC\": m_hgb[\"roc_auc\"], \"PR_AUC\": m_hgb[\"pr_auc\"], \"Brier\": m_hgb[\"brier\"]},\n",
    "]).sort_values(\"ROC_AUC\", ascending=False)\n",
    "\n",
    "summary"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}